{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "national_parks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BtGISGMrGT9"
      },
      "source": [
        "#importing required libraries\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "#import time\n",
        "\n",
        "#start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ctl02fasRLT"
      },
      "source": [
        "def find_phone(park_soup):\n",
        "#function to find phone number from each park's page (page 3)\n",
        "#input is page soup\n",
        "#output is phone number in string format\n",
        "  phone_loc = park_soup.find(\"h4\", text=\"Phone:\")                                 #find text 'phone:' with tag <h4>\n",
        "  if phone_loc:                                                                   \n",
        "    park_ph = phone_loc.findNextSibling('p').get_text().strip('\\n')               #extract the phone number and remove unwanted '\\n'\n",
        "    park_ph = park_ph.split(\"\\n\", 1)[0] #.replaceAll(\"\\\\D+\", \"\")                  #some phone numbers are followed by text, keep only the phone number\n",
        "  else:\n",
        "    park_ph = ''                                                                  #condition if no phone number stated\n",
        "  return park_ph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c-AdSXAuSr5"
      },
      "source": [
        "def find_add(soup):\n",
        "#function to find the complete park address from each park's page (page 3)\n",
        "#input is page soup\n",
        "#output is address line 1, 2, 3, city, state, zip code as a list\n",
        "\n",
        "  add_loc = soup.find(\"h4\", text=\"Mailing Address:\")\n",
        "  if add_loc:\n",
        "    park_add = add_loc.findNextSibling('div').find('span',\n",
        "                                                     itemprop=\"streetAddress\")    #extract mailing address tags\n",
        "  \n",
        "    if park_add:\n",
        "      park_add = park_add.text.strip('\\n')                                        #extract text from the tags\n",
        "      park_add_list = re.split('\\n', park_add)                                    #create a list of address with line 1, line 2, line 3 as 3 elements\n",
        "    else:\n",
        "      park_add_list = ['P.O. Box '+add_loc.findNextSibling('div')\\\n",
        "                       .find('span', itemprop=\"postOfficeBoxNumber\")\\\n",
        "                       .text.strip('\\n')]                                         #extract PO box number if no address\n",
        "\n",
        "    if len(park_add_list)==1:                                                     #assign address to variables\n",
        "      park_add_1 = park_add_list[0]\n",
        "      park_add_2 = ''\n",
        "      park_add_3 = ''\n",
        "    elif len(park_add_list)==2:\n",
        "      park_add_1 = park_add_list[0]\n",
        "      park_add_2 = park_add_list[1]\n",
        "      park_add_3 = '' \n",
        "    else:\n",
        "      park_add_1 = park_add_list[0]\n",
        "      park_add_2 = park_add_list[1]\n",
        "      park_add_3 = park_add_list[2]\n",
        "\n",
        "    park_city = add_loc.findNextSibling('div').find('span',\n",
        "                                                    itemprop='addressLocality')\\\n",
        "                                                    .text.strip('\\n')             #extract and assign city name\n",
        "    park_state = add_loc.findNextSibling('div').find('span',\n",
        "                                                     itemprop='addressRegion')\\\n",
        "                                                     .text.strip('\\n')            #extract and assign state name\n",
        "    park_zip = add_loc.findNextSibling('div').find('span',\n",
        "                                                   itemprop='postalCode')\\\n",
        "                                                   .text.strip('\\n')              #extract and assign zip code\n",
        "  else:                                                                           #assign empty string if no address found\n",
        "    park_add_1 = ''\n",
        "    park_add_2 = ''\n",
        "    park_add_3 = ''\n",
        "    park_city = ''\n",
        "    park_state = ''\n",
        "    park_zip = ''\n",
        "\n",
        "  return [park_add_1, park_add_2, park_add_3, park_city, park_state, park_zip]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfLKCVzPyvlv"
      },
      "source": [
        "def find_park_data(state_link):\n",
        "#This function extracts all the data/attributes of all the parks of the state from the state's page (page 2)\n",
        "#input is the state's link\n",
        "#Output is a dataframe containing all attributes of the parks\n",
        "  state_page = requests.get(state_link)\n",
        "  state_soup = BeautifulSoup(state_page.content, 'html.parser')                   ###Retrieve each state’s page (page 2)\n",
        "  park_loc = state_soup.find(\"h2\", text=\"Parks\")                                  #find the \"parks\" with <h2> tag, all the park categories follow in <h2>\n",
        "\n",
        "###Retrieve each park’s name, category, and description from\n",
        "###the list of parks in each state.\n",
        "#finding park categories\n",
        "  park_cat = park_loc.parent.parent.findChildren('h2')\n",
        "  park_cats = []                                                                  #initialize empty list for park category, name, link and description\n",
        "  park_names = []\n",
        "  park_links = []\n",
        "  park_descs = []\n",
        "  for cat in park_cat:\n",
        "      park_cat = cat.findNext('h2').text                                          #extract text from the tag, append and store in list park_cats\n",
        "      park_cats.append(park_cat)\n",
        "  park_cats = park_cats[:-1]                                                      #gives an extra unwanted row with unwanted text,\n",
        "                                                                                  #modifying to get desired number of rows\n",
        "#finding park links and names\n",
        "  park_link = park_loc.parent.parent.findChildren('h3')                           #finding <h3> tag which holds the href and names of the park\n",
        "  for nlink in park_link:\n",
        "      link = nlink.findChild('a')\n",
        "      name=link.text\n",
        "      park_links.append(link['href'])                                             #extract and append to list park_links and park_names\n",
        "      park_names.append(name)                                                     ###Retrieve the list of all parks from page 2 for each state\n",
        "\n",
        "  park_links=[weblink+line for line in park_links]                                #modifying relative link to full web link\n",
        "\n",
        "#extracting and storing park description\n",
        "  park_desc=park_loc.parent.parent.findChildren('p')\n",
        "  for ndesc in park_desc:\n",
        "    desc = ndesc.get_text().strip('\\n')\n",
        "    park_descs.append(desc)\n",
        "\n",
        "#creating soup for each park's page (page 3) for current state\n",
        "  park_pages = [requests.get(park_page) for park_page in park_links]              ###Retrieve each park’s page (page 3)\n",
        "  park_soups = [BeautifulSoup(park_soup.content,\n",
        "                              'html.parser') for park_soup in park_pages]\n",
        "\n",
        "#extracting and storing park phone numbers using the find_phone() function        ###Retrieve each park’s contact information from page 3 for each park.\n",
        "  park_phs = [find_phone(park_soups[i])\\\n",
        "              for i in range(len(park_links))]\n",
        "\n",
        "#extracting and storing park addresses using the find_add() function              \n",
        "  park_add_1 = [find_add(park_soups[i])[0]\\\n",
        "                for i in range(len(park_links))]\n",
        "  park_add_2 = [find_add(park_soups[i])[1]\\\n",
        "                for i in range(len(park_links))]\n",
        "  park_add_3 = [find_add(park_soups[i])[2]\\\n",
        "                for i in range(len(park_links))]\n",
        "  park_city = [find_add(park_soups[i])[3]\\\n",
        "               for i in range(len(park_links))]\n",
        "  park_state = [find_add(park_soups[i])[4]\\\n",
        "                for i in range(len(park_links))]\n",
        "  park_zip = [find_add(park_soups[i])[5]\\\n",
        "              for i in range(len(park_links))]\n",
        "\n",
        "#storing all the attributes to a dataframe \n",
        "  df=pd.DataFrame(np.column_stack([park_names,park_cats,park_descs,\n",
        "                                   park_add_1,park_add_2,park_add_3,park_city,\n",
        "                                   park_state,park_zip,park_phs]),\n",
        "                   columns=['Name','Category','Description',\n",
        "                            'Street Address Line 1','Line 2','Line 3',\n",
        "                            'City','State','Zip Code','Phone Number'])\n",
        "  \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuXxfHdIEMmr"
      },
      "source": [
        "weblink='https://www.nps.gov'\n",
        "page = requests.get(weblink)\n",
        "soup = BeautifulSoup(page.content, 'html.parser')                                 ###Retrieve the page with the drop-down list (page 1)\n",
        "state_loc = soup.find(\"a\", text=\"Alabama\")\n",
        "state_pages = state_loc.parent.findNextSiblings('li')\n",
        "links = [state_loc['href']]\n",
        "for e_page in state_pages:\n",
        "    link = e_page.findChild('a')\n",
        "    links.append(link['href'])\n",
        "state_links=[weblink+line for line in links]                                      ###Retrieve the links to all states from page 1\n",
        "\n",
        "park_data=[]\n",
        "for i in range(len(state_links)):\n",
        "  park_data.append(find_park_data(state_links[i]))\n",
        "\n",
        "parks_data = pd.concat(park_data)\n",
        "#parks_data.remove_duplicates()                                                   #if duplicates are not to be included\n",
        "#dup=parks_data[parks_data.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twb6XnUOJJz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e21e95-60c5-49c3-b63b-66d6edd4cc12"
      },
      "source": [
        "parks_data.reset_index(drop=True, inplace=True)\n",
        "parks_data.to_csv('nation_parks.csv')                                             ###results stored in csv file\n",
        "#print(\"%s seconds\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127.91666388511658 seconds\n"
          ]
        }
      ]
    }
  ]
}