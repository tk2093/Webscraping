{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dasc5301-fall2021-p1-solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yQGrFvLlaXb"
      },
      "source": [
        "# DASC5301 Data Science, Fall 2021, Chengkai Li, Unversity of Texas at Arlington\n",
        "# Programming Assignment 1 Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC6bWoktjxMt"
      },
      "source": [
        "import requests\n",
        "page = requests.get(\"https://www.nps.gov/robots.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbr_g3alfeQm"
      },
      "source": [
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as dates\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60CVMwT-tPOF"
      },
      "source": [
        "#Retrieve the first page.\n",
        "page = requests.get(\"https://www.nps.gov/index.htm\")\n",
        "soup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBSK0Uknvrtq"
      },
      "source": [
        "#Find the list of all states.\n",
        "text_loc = soup.find(\"a\", text=\"Alabama\")\n",
        "text_loc = text_loc.parent\n",
        "all_states_li = text_loc.findNextSiblings(\"li\")\n",
        "\n",
        "all_states_a = []\n",
        "tmp_a = text_loc.findChild('a')\n",
        "all_states_a.append(tmp_a)\n",
        "\n",
        "for li in all_states_li: \n",
        "    tmp_a = li.findChild('a')\n",
        "    all_states_a.append(tmp_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Zu36e6GWUf"
      },
      "source": [
        "#Extract each park's contact info. \n",
        "def park_info(park_link):\n",
        "    page = requests.get(park_link)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    text_loc = soup.find('h4', text=\"Mailing Address:\")\n",
        "    if text_loc == None: \n",
        "        address = \" \"\n",
        "    else:\n",
        "        address_div = text_loc.findNextSibling('div')\n",
        "        if address_div == None: \n",
        "            address_p = text_loc.findNextSibling('p')\n",
        "            address = \" \" + address_p.text\n",
        "        else: \n",
        "            address_p = address_div.findChild('p')\n",
        "            address_spans = address_p.findChildren('span')\n",
        "            address = \" \" + address_spans[0].text + address_spans[1].text\n",
        "\n",
        "    text_loc = soup.find(\"h4\", text=\"Phone:\")\n",
        "    if text_loc == None: \n",
        "        park_phone = \" \"\n",
        "    else: \n",
        "        phone_p = text_loc.findNextSibling('p')\n",
        "        phone_span = phone_p.findChild('span')\n",
        "        park_phone = phone_span.text\n",
        "    \n",
        "    #Address\n",
        "    addressLine1 = address\n",
        "    addressLine2 = \" \"\n",
        "    addressLine3 = \" \"\n",
        "    city = \" \"\n",
        "    state = \" \"\n",
        "    zipcode = \" \"\n",
        "\n",
        "    if address != '' and  not address.isspace(): \n",
        "      addressLines = address.split('\\n')\n",
        "      if addressLines[-1] == ' ' or addressLines[-1] == '': \n",
        "        addressLines = addressLines[:-1]\n",
        "      if addressLines[0] == ' ' or addressLines[0] == '':\n",
        "        addressLines = addressLines[1:]\n",
        "      \n",
        "      if len(addressLines) >= 3: \n",
        "        zipcode = addressLines[-1]\n",
        "        city_state = addressLines[-2].split(',')\n",
        "        city = city_state[0]\n",
        "        state = city_state[1]\n",
        "        addressLine1 = addressLines[0]\n",
        "      if len(addressLines) >= 4: \n",
        "        addressLine2 = addressLines[1]\n",
        "      if len(addressLines) >= 5: \n",
        "        addressLine3 = addressLines[2]\n",
        "\n",
        "    return addressLine1, addressLine2, addressLine3, city, state, zipcode, park_phone "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDT913w60C3z"
      },
      "source": [
        "parks_list = []\n",
        "#Retrieve all states pages.\n",
        "for a in all_states_a: \n",
        "    state = a.text\n",
        "    href = a['href']\n",
        "    state_link = \"https://www.nps.gov\" + href\n",
        "    page = requests.get(state_link)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    text_loc = soup.find('ul', id=\"list_parks\")\n",
        "    all_h2s = text_loc.findAllNext('h2')\n",
        "    #Retrieve parks pages. \n",
        "    for h2 in all_h2s:\n",
        "        park_category = h2.text\n",
        "        h3_name = h2.findNext('h3')\n",
        "        if h3_name == None:\n",
        "            continue\n",
        "        a_name = h3_name.findChild('a')\n",
        "        if a_name == None: \n",
        "            continue\n",
        "        park_link = \"http://www.nps.gov\" + a_name['href'] + \"index.htm\"\n",
        "        if park_link == \"http://www.nps.gov/es-es/mora/index.htm\": \n",
        "            continue\n",
        "        park_name = a_name.text\n",
        "        p_desc = h2.findNext('p')\n",
        "        park_desc = p_desc.text\n",
        "      \n",
        "        addressLine1, addressLine2, addressLine3, city, state, zipcode, park_phone = park_info(park_link)\n",
        "        parks_list.append([park_name, state, park_category, park_desc, addressLine1, addressLine2, addressLine3, city, state, zipcode, park_phone])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGKe0Ig7AtIh"
      },
      "source": [
        "#Saving the data into a csv file.\n",
        "df = pd.DataFrame(parks_list, columns = ['name', 'state', 'category','Description', 'addressLine1', 'addressLine2', 'addressLine3', 'city', 'state', 'zipcode', 'phone'])\n",
        "df.to_csv('parks_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}